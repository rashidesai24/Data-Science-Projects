{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNTxCoCzEKdMEcm6C4D6s4N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rashidesai24/Data-Science-Projects/blob/master/PySpark%20with%20Amazon%20Tweets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUUMuGKGU1rW",
        "outputId": "808c2dad-0139-46bb-fb57-05175a75089e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fS_uzKjqVvaA"
      },
      "source": [
        "**Installing Spark**\n",
        "\n",
        "To run spark in Colab, we need to first install all the dependencies in Colab environment i.e. Apache Spark 2.3.2 with hadoop 2.7, Java 8 and Findspark to locate the spark in the system. The tools installation can be carried out inside the Colab notebook. Code reference:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvMWQ5h3g6Rw"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null   #Installing Spark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUrRuT13g6U2"
      },
      "source": [
        "!wget -q https://www-us.apache.org/dist/spark/spark-2.4.7/spark-2.4.7-bin-hadoop2.6.tgz # Get Spark Installer version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6e5hXJwg6YI"
      },
      "source": [
        "!tar xf spark-2.4.7-bin-hadoop2.6.tgz # Untarring Spark Installer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NFiDSyqg6bH"
      },
      "source": [
        "!pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4BeiU23iB4r"
      },
      "source": [
        "Once Spark and Java are installed in Colab, we set an environment path that enables to run Pyspark in the Colab environment. Setting the location of Java and Spark:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7FjwRNhg6cf"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.7-bin-hadoop2.6\""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VXfMeMlk4Zb"
      },
      "source": [
        "Running a local  session to test the Spark installation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSAA60BnioN_"
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "# Colab is ready to run Pyspark!"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3x5epky7lHeX"
      },
      "source": [
        "Importing Amazon Responded data and selecting the required columns\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVgXjHniiocN"
      },
      "source": [
        "data = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"drive/My\\ Drive/Colab\\ Notebooks/Amazon_Responded_Oct05.csv\")\n",
        "data1 = data.select(\"id_str\",'tweet_created_at','user_verified','favorite_count','retweet_count','text_')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XL-QRp-bmc5P"
      },
      "source": [
        "***Step 1: Remove the records where “user_verified” is “FALSE”.***\n",
        "\n",
        "We will only keep the records where user_verified is TRUE: verified tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nzi0J_4zokyb",
        "outputId": "22b6b739-8baa-4bc3-c5c9-b794065cfe34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data_filtered = data1.filter(data1.user_verified == 'False')\n",
        "data_filtered.count() # Counting the number of tweets whereverified users"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "193220"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iOIYexpisne",
        "outputId": "209a7bff-7039-4644-ee38-ffba149a644a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data_filtered = data1.filter(data1.user_verified == 'True')\n",
        "data_filtered.count() # Counting the number of tweets with only vaild/verified users\n",
        "# Keeping the records where “user_verified” is True."
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "171797"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDkE4-Hyisqh",
        "outputId": "dfaab484-7f8f-439f-ab04-51f7c91b2f23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "data_filtered.select(\"tweet_created_at\").show(5,False) #Displaying the top 5 rows for tweet times from the \"tweet_created_at\" column"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------------------------+\n",
            "|tweet_created_at              |\n",
            "+------------------------------+\n",
            "|Tue Nov 01 02:39:55 +0000 2016|\n",
            "|Tue Nov 01 17:19:57 +0000 2016|\n",
            "|Tue Nov 01 17:25:26 +0000 2016|\n",
            "|Tue Nov 01 18:02:03 +0000 2016|\n",
            "|Tue Nov 01 03:59:02 +0000 2016|\n",
            "+------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eyhVXw0oOj8"
      },
      "source": [
        "Splitting the date column to parse month date and year"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZkX_PAaissy",
        "outputId": "4a3b6fb1-e6d8-4942-b5a1-80a18c411f9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "from pyspark.sql.functions import split   \n",
        "tweet = split(data_filtered[\"tweet_created_at\"], ' ')\n",
        "data_filtered1 = data_filtered.withColumn('Month', tweet.getItem(1))\n",
        "data_filtered1 = data_filtered1.withColumn('Date', tweet.getItem(2))\n",
        "data_filtered1 = data_filtered1.withColumn('Year', tweet.getItem(5))\n",
        "data_filtered1.show(5)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+-------------+--------------+-------------+--------------------+-----+----+----+\n",
            "|              id_str|    tweet_created_at|user_verified|favorite_count|retweet_count|               text_|Month|Date|Year|\n",
            "+--------------------+--------------------+-------------+--------------+-------------+--------------------+-----+----+----+\n",
            "|'793281386912354304'|Tue Nov 01 02:39:...|         True|             0|            0|@SeanEPanjab I'm ...|  Nov|  01|2016|\n",
            "|'793502854459879424'|Tue Nov 01 17:19:...|         True|             0|            0|@SeanEPanjab Plea...|  Nov|  01|2016|\n",
            "|'793504235400884224'|Tue Nov 01 17:25:...|         True|             0|            0|@SeanEPanjab With...|  Nov|  01|2016|\n",
            "|'793513446633533440'|Tue Nov 01 18:02:...|         True|             0|            0|@SeanEPanjab I'm ...|  Nov|  01|2016|\n",
            "|'793301295255945216'|Tue Nov 01 03:59:...|         True|             0|            0|@aakashwangnoo Hi...|  Nov|  01|2016|\n",
            "+--------------------+--------------------+-------------+--------------+-------------+--------------------+-----+----+----+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lR0x5V7DisvX",
        "outputId": "84d40b0a-92cb-45fb-9772-ade88582fed0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "import pyspark.sql.functions as sq \n",
        "df = data_filtered1.withColumn(\"tweet_created_at\",sq.concat(sq.col(\"Month\"), sq.lit(\" \"), sq.col(\"Date\"),sq.lit(\" \"), sq.col(\"Year\")))\n",
        "df = df.select('id_str','tweet_created_at','user_verified',df.favorite_count.cast('int'), df.retweet_count.cast('int'),'text_')\n",
        "df.show(5) # Show only the first 5 rows"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+----------------+-------------+--------------+-------------+--------------------+\n",
            "|              id_str|tweet_created_at|user_verified|favorite_count|retweet_count|               text_|\n",
            "+--------------------+----------------+-------------+--------------+-------------+--------------------+\n",
            "|'793281386912354304'|     Nov 01 2016|         True|             0|            0|@SeanEPanjab I'm ...|\n",
            "|'793502854459879424'|     Nov 01 2016|         True|             0|            0|@SeanEPanjab Plea...|\n",
            "|'793504235400884224'|     Nov 01 2016|         True|             0|            0|@SeanEPanjab With...|\n",
            "|'793513446633533440'|     Nov 01 2016|         True|             0|            0|@SeanEPanjab I'm ...|\n",
            "|'793301295255945216'|     Nov 01 2016|         True|             0|            0|@aakashwangnoo Hi...|\n",
            "+--------------------+----------------+-------------+--------------+-------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIL12tchoWgO"
      },
      "source": [
        "***Step 2: For the records (“user_verified” is “TRUE”), group by created date, and count the number of tweets for each date***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypyNz3Tii1wl",
        "outputId": "1b1409d2-10e0-4871-d5cd-f115767571fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "tweet_count = df.groupby(df.tweet_created_at).agg(sq.count('id_str').alias(\"count_of_tweets\"))\n",
        "tweet_count.show(10)\n",
        "# Output will be the dates of the tweets created by date and the count"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------------+---------------+\n",
            "|tweet_created_at|count_of_tweets|\n",
            "+----------------+---------------+\n",
            "|     Dec 01 2016|            875|\n",
            "|     Dec 25 2016|            433|\n",
            "|     Feb 08 2017|            926|\n",
            "|     Feb 19 2017|            725|\n",
            "|     Mar 30 2017|           1109|\n",
            "|     Apr 26 2017|            295|\n",
            "|     Dec 05 2016|            824|\n",
            "|     Dec 18 2016|            178|\n",
            "|     Jul 06 2017|            673|\n",
            "|     Mar 20 2017|            558|\n",
            "+----------------+---------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8JP_WOIi11D",
        "outputId": "07bd794d-cca8-43e9-df54-8b63e9961588",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "import pyspark\n",
        "conf = pyspark.SparkConf()\n",
        "sc = pyspark.SparkContext.getOrCreate(conf=conf)\n",
        "from pyspark.sql import SQLContext\n",
        "sqlcontext = SQLContext(sc)\n",
        "\n",
        "tweet_count.registerTempTable(\"tmpcounts\")\n",
        "tweet_count_ordered = sqlcontext.sql(\"SELECT * FROM tmpcounts order by count_of_tweets desc limit 10\")\n",
        "tweet_count_ordered.show()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------------+---------------+\n",
            "|tweet_created_at|count_of_tweets|\n",
            "+----------------+---------------+\n",
            "|     Jan 03 2017|           1536|\n",
            "|     Jan 10 2017|           1508|\n",
            "|     Jan 11 2017|           1496|\n",
            "|     Jan 12 2017|           1410|\n",
            "|     Jan 06 2017|           1364|\n",
            "|     Jan 07 2017|           1360|\n",
            "|     Jan 20 2017|           1336|\n",
            "|     Mar 02 2017|           1296|\n",
            "|     Jan 13 2017|           1295|\n",
            "|     Jan 21 2017|           1290|\n",
            "+----------------+---------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jgiIIrKwEsn"
      },
      "source": [
        "***Step 3: For the date with highest number of tweets (you can figure it out from step 2), calculate the sum of “favorite_count” and “retweet_count” for each tweet on that day. Then report the text content (“text_”) of the top 100 tweets with highest sum. Count the word frequency of the 100 tweets and report the result.***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JO6un_Ppi15-",
        "outputId": "cbe3ddbe-9cd9-45fa-8771-134744cabcf1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "df.registerTempTable(\"tmpdf\")\n",
        "# Writing a SQL query to retrieve 100 favorite count and retweet count where \n",
        "sum_of_retweets = sqlcontext.sql(\"SELECT text_,favorite_count+ retweet_count as total from tmpdf where tweet_created_at = 'Jan 03 2017' order by total desc limit 100\")\n",
        "sum_of_retweets.show(10,False)\n",
        "text = sum_of_retweets.toPandas()\n",
        "a =text[\"text_\"].tolist()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
            "|text_                                                                                                                                       |total|\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
            "|@amazon worst shopping  experience,  no service, no substantial reply to complaints, no delivery for 1 week post guarantee date.            |5    |\n",
            "|@ItsJosshA We always aim to deliver by the date given in your confirmation email. Have we missed that date? Any update in tracking?  ^NF    |3    |\n",
            "|@ItsJosshA Oh no! I'm sorry! Please reach out to us so that we can look into options: https://t.co/hApLpMlfHN. ^JO                          |2    |\n",
            "|@KStefl Sounds like you know what to add to your Halloween playlist for this year! ^BV                                                      |2    |\n",
            "|@Schoey1992 Happy Birthday, Matt! #FriendsForever #FriendshipGoals ^JO                                                                      |2    |\n",
            "|@ratbones666 You so fancy!!! You already knooow!!! ^EP                                                                                      |2    |\n",
            "|@ThorpPerrow Awww! Happy Birthday, you don't look a day over 20 ;-) .Pass on your details here: https://t.co/g52MRb0AOt for lil surprise ^RS|2    |\n",
            "|@thedexterouz Hi! We'd like to help. When you have a moment, please connect with us here:  https://t.co/vlvfJr4nN9 ^YP                      |2    |\n",
            "|@matt_linsley Please keep us posted on this and let us know if it doesn't arrive tomorrow. ^BV                                              |1    |\n",
            "|@VlSlT Sorry to hear that, contact us here: https://t.co/hApLpMlfHN and we'll look into it for you. ^AS                                     |1    |\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9PdEw-hyGp7"
      },
      "source": [
        "As a good practice to the data processing, it is important that we clean the data. For the same, defining a function remove punctuation, extra symbols from the usernames"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKe79cvbisyA",
        "outputId": "eedf38d7-2bfc-4fbf-b199-698b937fb00c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Importing required libraries\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "nltk.download('punkt') # Downloading package punkt\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "nltk.download('stopwords')\n",
        "from collections import Counter\n",
        "\n",
        "def cleaning(textfile):\n",
        "    y = textfile\n",
        "    cleaned=[]\n",
        "    for i in y:\n",
        "      # print(i)\n",
        "      i.replace(\"'\", \" \")  #step 1: replacing apostraphe\n",
        "      i=re.sub(r\"\\@\\w+\",\" \",i)   #step2: removing words starting with @\n",
        "      cleaned.append(i.translate(str.maketrans(string.punctuation,' '*len(string.punctuation)))) # step 3: remove_punctuation                   \n",
        "    output=[]\n",
        "    for i in cleaned:\n",
        "        output.append(\" \".join([w.lower() for w in i.split()  if w.isalpha()]))    \n",
        "\n",
        "    return output\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1TmGvdCjCq9",
        "outputId": "36d302bd-4d5c-4328-c2cc-4cf3437283cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "clean_text = cleaning(a)\n",
        "from pyspark import SparkContext\n",
        "sc =SparkContext.getOrCreate()\n",
        "k = clean_text\n",
        "tuple(k)\n",
        "rdd = sc.parallelize(k)\n",
        "result_rdd = rdd.flatMap(lambda line: line.split(\" \")) \\\n",
        "    .map(lambda word: (word, 1)) \\\n",
        "    .reduceByKey(lambda a, b: a + b) \\\n",
        "    .collect()\n",
        "print(result_rdd)\n",
        "tweet_result_rdd=pd.DataFrame(result_rdd)\n",
        "tweet_result_rdd.to_csv(r\"/content/tweet_result_rdd.csv\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('worst', 1), ('no', 5), ('service', 1), ('substantial', 1), ('delivery', 6), ('week', 1), ('we', 46), ('always', 3), ('in', 13), ('confirmation', 1), ('have', 18), ('update', 2), ('tracking', 5), ('fancy', 1), ('already', 1), ('don', 5), ('look', 7), ('pass', 2), ('details', 6), ('https', 44), ('co', 45), ('lil', 1), ('sounds', 1), ('like', 7), ('know', 13), ('playlist', 1), ('this', 27), ('year', 2), ('bv', 2), ('d', 7), ('help', 9), ('when', 5), ('connect', 1), ('us', 28), ('matt', 1), ('friendshipgoals', 1), ('oh', 1), ('i', 23), ('out', 6), ('into', 6), ('options', 3), ('hear', 4), ('as', 4), ('let', 7), ('doesn', 5), ('arrive', 6), ('tomorrow', 2), ('an', 8), ('ar', 3), ('apologies', 2), ('incorrect', 1), ('item', 5), ('using', 2), ('above', 2), ('do', 4), ('further', 5), ('concerns', 1), ('ca', 1), ('thanks', 11), ('shout', 1), ('looking', 2), ('improve', 1), ('share', 1), ('specific', 1), ('feedback', 7), ('via', 4), ('form', 1), ('sj', 6), ('delay', 1), ('indicate', 1), ('new', 3), ('check', 1), ('updates', 3), ('amazon', 7), ('is', 13), ('est', 1), ('at', 6), ('checkout', 1), ('late', 2), ('cyqkduakwj', 2), ('af', 2), ('s', 7), ('now', 3), ('identify', 1), ('flagging', 1), ('reviewed', 1), ('earliest', 1), ('account', 2), ('but', 3), ('refund', 2), ('en', 3), ('was', 5), ('sent', 1), ('fulfilled', 1), ('third', 2), ('site', 3), ('doug', 1), ('returns', 1), ('items', 1), ('online', 1), ('more', 3), ('seeing', 1), ('advise', 1), ('there', 2), ('getting', 2), ('below', 1), ('pk', 1), ('truly', 2), ('only', 1), ('able', 3), ('end', 1), ('of', 3), ('enjoying', 2), ('collection', 1), ('keeps', 1), ('binge', 1), ('watch', 1), ('feel', 4), ('athena', 1), ('rather', 2), ('delivering', 2), ('things', 1), ('than', 1), ('cold', 1), ('medicine', 1), ('am', 1), ('customer', 2), ('sr', 5), ('card', 2), ('typically', 3), ('process', 1), ('business', 2), ('days', 1), ('requested', 2), ('ve', 2), ('connecting', 1), ('leave', 3), ('way', 2), ('ab', 1), ('waiting', 1), ('carriers', 1), ('until', 2), ('shipment', 1), ('transit', 1), ('destination', 1), ('edd', 1), ('promised', 1), ('inconvenience', 2), ('caused', 1), ('alerts', 2), ('forwarded', 1), ('relevant', 1), ('department', 2), ('jj', 1), ('trouble', 3), ('tried', 1), ('unlinking', 1), ('linking', 1), ('cs', 1), ('sound', 1), ('right', 1), ('support', 3), ('availability', 1), ('payment', 2), ('couriers', 1), ('make', 2), ('sure', 4), ('regret', 1), ('work', 1), ('these', 4), ('instances', 1), ('never', 1), ('happen', 1), ('bringing', 2), ('page', 1), ('investigate', 1), ('hb', 4), ('receiver', 1), ('stopped', 1), ('working', 1), ('very', 1), ('interest', 2), ('phone', 3), ('longer', 1), ('visit', 2), ('mobile', 1), ('browser', 1), ('where', 2), ('newest', 1), ('following', 2), ('steps', 4), ('sb', 1), ('understand', 1), ('showing', 1), ('allow', 1), ('free', 2), ('chat', 2), ('assistance', 1), ('team', 2), ('send', 1), ('use', 1), ('desktop', 1), ('cd', 1), ('concern', 1), ('filled', 1), ('mm', 1), ('didn', 1), ('ask', 1), ('are', 2), ('quality', 1), ('shirt', 1), ('contacted', 2), ('them', 1), ('ra', 1), ('delighted', 1), ('appreciation', 1), ('stayhealthy', 1), ('friendly', 1), ('love', 1), ('certainly', 1), ('try', 3), ('danny', 1), ('may', 2), ('open', 1), ('settings', 1), ('kd', 1), ('tm', 2), ('john', 1), ('recommend', 1), ('website', 1), ('social', 1), ('gl', 3), ('confirm', 1), ('questions', 2), ('li', 1), ('start', 1), ('puppy', 1), ('pretty', 1), ('cute', 1), ('sharing', 1), ('becoming', 1), ('joke', 1), ('speak', 1), ('trying', 1), ('put', 1), ('cutting', 1), ('eligible', 1), ('edit', 1), ('orders', 1), ('letting', 2), ('acct', 1), ('books', 1), ('select', 1), ('vihgdspeur', 1), ('goes', 1), ('mark', 1), ('anything', 1), ('night', 1), ('close', 1), ('click', 1), ('quickly', 1), ('regarding', 1), ('issues', 1), ('hasn', 1), ('qualify', 1), ('claim', 1), ('give', 1), ('other', 1), ('reported', 1), ('looked', 1), ('shopping', 4), ('experience', 9), ('reply', 2), ('to', 65), ('complaints', 1), ('for', 50), ('post', 1), ('guarantee', 3), ('date', 5), ('aim', 2), ('deliver', 4), ('by', 8), ('the', 65), ('given', 1), ('your', 34), ('email', 3), ('missed', 1), ('that', 20), ('any', 8), ('nf', 1), ('you', 64), ('so', 18), ('knooow', 1), ('ep', 2), ('awww', 1), ('happy', 5), ('birthday', 2), ('t', 61), ('a', 25), ('day', 4), ('over', 1), ('on', 14), ('here', 33), ('surprise', 1), ('rs', 2), ('what', 6), ('add', 1), ('halloween', 1), ('hi', 14), ('moment', 1), ('please', 20), ('with', 6), ('yp', 2), ('friendsforever', 1), ('jo', 6), ('m', 13), ('sorry', 21), ('reach', 3), ('can', 28), ('haplpmlfhn', 5), ('contact', 11), ('and', 17), ('ll', 9), ('it', 19), ('keep', 7), ('posted', 6), ('if', 22), ('they', 1), ('direct', 1), ('number', 1), ('bad', 2), ('request', 4), ('return', 5), ('link', 6), ('provided', 3), ('sh', 6), ('hey', 9), ('glad', 3), ('issue', 2), ('has', 3), ('been', 2), ('resolved', 1), ('re', 10), ('pls', 3), ('did', 1), ('e', 2), ('mail', 2), ('order', 10), ('need', 4), ('download', 2), ('video', 2), ('app', 6), ('which', 1), ('separate', 1), ('from', 2), ('how', 3), ('report', 2), ('not', 4), ('cl', 2), ('up', 2), ('ak', 1), ('access', 3), ('through', 3), ('twitter', 2), ('review', 4), ('wrong', 2), ('case', 2), ('being', 1), ('or', 4), ('party', 2), ('seller', 5), ('our', 10), ('arrange', 1), ('most', 3), ('see', 3), ('info', 4), ('cj', 2), ('aren', 1), ('contacting', 1), ('error', 1), ('while', 2), ('redeeming', 1), ('rw', 1), ('directly', 1), ('want', 2), ('customers', 1), ('be', 8), ('aw', 1), ('prime', 1), ('better', 3), ('stay', 1), ('tuned', 1), ('soon', 2), ('much', 2), ('fun', 1), ('features', 1), ('refunds', 1), ('credit', 1), ('within', 1), ('received', 1), ('shall', 1), ('about', 4), ('skgzkjcdng', 1), ('went', 1), ('could', 1), ('elaborate', 1), ('arrived', 3), ('exchange', 2), ('find', 2), ('damaged', 1), ('replacement', 3), ('nxgaalzhis', 1), ('will', 6), ('pm', 1), ('then', 3), ('wj', 4), ('scan', 3), ('indicates', 1), ('should', 2), ('manage', 2), ('text', 1), ('spotify', 1), ('alexa', 1), ('time', 3), ('unable', 2), ('mpos', 1), ('device', 1), ('cod', 1), ('vs', 1), ('create', 1), ('stock', 2), ('same', 1), ('place', 1), ('its', 1), ('isn', 2), ('repeated', 1), ('attention', 2), ('list', 1), ('me', 3), ('next', 1), ('thank', 1), ('windows', 1), ('application', 1), ('available', 2), ('encourage', 1), ('cancel', 1), ('membership', 1), ('hope', 3), ('helps', 3), ('concerned', 1), ('expected', 1), ('status', 2), ('some', 3), ('real', 1), ('td', 1), ('kindle', 3), ('teams', 2), ('assist', 2), ('hg', 2), ('ctrl', 1), ('f', 1), ('address', 1), ('get', 3), ('kindly', 1), ('drop', 1), ('shared', 1), ('earlier', 2), ('asked', 1), ('receive', 1), ('reviews', 1), ('written', 1), ('w', 2), ('purchased', 1), ('product', 1), ('happynewyear', 1), ('packages', 1), ('ja', 3), ('shoutout', 1), ('knowing', 1), ('helped', 1), ('updated', 1), ('sorted', 2), ('ae', 3), ('menu', 1), ('packaging', 1), ('submit', 1), ('amp', 1), ('picture', 1), ('checking', 1), ('media', 1), ('back', 1), ('frustrating', 1), ('reading', 1), ('lg', 1), ('paw', 1), ('supervisor', 1), ('colleagues', 1), ('off', 1), ('still', 1), ('modify', 1), ('method', 1), ('great', 2), ('evening', 1), ('else', 1), ('yourself', 1), ('recent', 1), ('location', 1), ('cleared', 1), ('customs', 1), ('view', 1), ('were', 1), ('ship', 1), ('fitbit', 1), ('arrives', 1), ('information', 1), ('rm', 1), ('z', 1), ('found', 1), ('had', 1), ('internally', 1), ('patience', 1), ('hk', 1)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbbmnHTE4G3l"
      },
      "source": [
        "# TASK 2\n",
        "Join the text content of each tweet according to “id_str” to Amazon_Responded_Oct05.csv and fill in the “text” column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pxYZSQvjCu5",
        "outputId": "82e654fe-6410-4d79-fdcb-5bf8de26e5e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# Loading and reading the find_text.csv file\n",
        "findtext_data= spark.read.format(\"csv\").option(\"header\",\"true\").load(\"drive/My\\ Drive/Colab\\ Notebooks/find_text.csv\")\n",
        "findtext_data.show(5,False)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+----+\n",
            "|id_str              |text|\n",
            "+--------------------+----+\n",
            "|'793270689780203520'|null|\n",
            "|'793281386912354304'|null|\n",
            "|'793299404975247360'|null|\n",
            "|'793301295255945216'|null|\n",
            "|'793315815411978240'|null|\n",
            "+--------------------+----+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEGyuvp9jCxS",
        "outputId": "42a1635f-da68-447b-9480-61a0b491adbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "# The output needs to have tweet IDs stitched to its corresponding text response from the Amazon Tweets text file.\n",
        "findtext_data.registerTempTable(\"ids\")\n",
        "data.registerTempTable(\"tweets\")\n",
        "out = sqlcontext.sql(\"SELECT DISTINCT I.id_str,T.text_ from ids I JOIN tweets T on I.id_str = T.id_str\")\n",
        "out.show(5,False)\n",
        "output = out.toPandas()\n",
        "output.to_csv(r\"/content/join_output.csv\") # Output the file as a CSV"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+-----------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|id_str              |text_                                                                                                                                    |\n",
            "+--------------------+-----------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|'793382930085253121'|@mybharatraj Hi! Sorry about that. I'd like our team to look into this, please reach out to us via: https://t.co/vlvfJr4nN9 ^SG          |\n",
            "|'793441656984903680'|@AmazonHelp done, please contact them and let them know I am waiting for my package.                                                     |\n",
            "|'793517259880861696'|Your customer service sucks, @AmazonHelp. If you guys can't do your job, perhaps go back to school and learn something proper.           |\n",
            "|'793533066157387776'|@flamablebrownie When checking your account, are you signed into the email that was used to place the order: https://t.co/aaDyEz1VgE? ^HB|\n",
            "|'793542659625349121'|So now @AmazonHelp is helping my hacker instead of helping me. Good job. 🙄 Why do you even have a help centre if you're of no help?     |\n",
            "+--------------------+-----------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}